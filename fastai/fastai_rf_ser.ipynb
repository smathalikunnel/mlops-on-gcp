{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in /opt/conda/lib/python3.7/site-packages (0.14)\n",
      "Requirement already satisfied: isoweek in /opt/conda/lib/python3.7/site-packages (1.3.3)\n",
      "Requirement already satisfied: pandas_summary in /opt/conda/lib/python3.7/site-packages (0.0.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from pandas_summary) (1.0.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pandas_summary) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->pandas_summary) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->pandas_summary) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->pandas_summary) (1.15.0)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.7/site-packages (7.15.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython) (2.6.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython) (0.1.0)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.7/site-packages (from ipython) (4.8.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython) (4.3.3)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython) (47.1.1.post20200529)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.7/site-packages (from ipython) (0.15.2)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython) (4.4.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython) (3.0.5)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython) (0.7.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython) (0.6.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from traitlets>=4.2->ipython) (1.15.0)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.7/site-packages (from traitlets>=4.2->ipython) (0.2.0)\n",
      "Requirement already satisfied: parso>=0.5.2 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.10->ipython) (0.5.2)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install graphviz\n",
    "\n",
    "!{sys.executable} -m pip install isoweek\n",
    "!{sys.executable} -m pip install pandas_summary\n",
    "!{sys.executable} -m pip install ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn-pandas\n",
      "  Using cached sklearn_pandas-1.8.0-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from sklearn-pandas) (0.23.1)\n",
      "Requirement already satisfied: pandas>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from sklearn-pandas) (1.0.4)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from sklearn-pandas) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.7/site-packages (from sklearn-pandas) (1.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.15.0->sklearn-pandas) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.15.0->sklearn-pandas) (0.15.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.11.0->sklearn-pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.11.0->sklearn-pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.11.0->sklearn-pandas) (1.15.0)\n",
      "Installing collected packages: sklearn-pandas\n",
      "Successfully installed sklearn-pandas-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install sklearn-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping sklearn-pandas as it is not installed.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip uninstall -y sklearn-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn                       0.23.1\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: scikit-learn 0.23.1\n",
      "Uninstalling scikit-learn-0.23.1:\n",
      "  Successfully uninstalled scikit-learn-0.23.1\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.4 MB 5.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.1)\n",
      "Installing collected packages: scikit-learn\n",
      "Successfully installed scikit-learn-0.20.4\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip uninstall -y scikit-learn\n",
    "!{sys.executable} -m pip install scikit-learn==0.20.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn                       0.20.4\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai_custom.imports import *\n",
    "from fastai_custom.structured import *\n",
    "\n",
    "from pandas_summary import DataFrameSummary\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn import metrics\n",
    "set_plot_sizes(12,14,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn                       0.23.1\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip list | grep scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn-pandas                     1.8.0\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip list | grep sklearn-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r gs://sk-kfp-kubeflowpipelines-eu/data/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PATH=os.path.join(os.getcwd(),'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(f'{PATH}/training/dataset.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all(df_raw.tail().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all(df_raw.describe(include='all').T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m = RandomForestClassifier(n_jobs=-1)\n",
    "#m.fit(df_raw.drop('Cover_Type', axis=1), df_raw.Cover_Type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical variables are currently stored as strings, which is inefficient, and doesn't provide the numeric coding required for a random forest. Therefore we call train_cats to convert strings to pandas categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data - converting strings to categories\n",
    "\n",
    "Same categories need to be applied to validation/testing data using ***apply_cats()***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cats(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.Wilderness_Area.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.Wilderness_Area.cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test time - supply the nas created by proc_df\n",
    "\n",
    "which columns are missing values, what are the median values of those columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display_all(df_raw.isnull().sum().sort_index()/len(df_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll replace categories with their numeric codes, handle missing continuous values, and split the dependent variable into a separate variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(f'{PATH}/validation/dataset.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, y, nas = proc_df(df_raw, 'Cover_Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30173, 12), (30173,), (9836, 12))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_vals(a,n): return a[:n].copy(), a[n:].copy()\n",
    "\n",
    "n_valid = 9836  # same as Kaggle's test set size\n",
    "n_trn = len(df)-n_valid\n",
    "raw_train, raw_valid = split_vals(df_raw, n_trn)\n",
    "X_train, X_valid = split_vals(df, n_trn)\n",
    "y_train, y_valid = split_vals(y, n_trn)\n",
    "\n",
    "X_train.shape, y_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(m):\n",
    "    res = [m.score(X_train, y_train), m.score(X_valid, y_valid)]\n",
    "    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.7817202114680765, 0.8708116528021741]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestClassifier(n_estimators=200, n_jobs=-1,oob_score=True,min_samples_leaf=1,max_features='sqrt')\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('model.pkl')\n",
    "pickle.dump(m, open('model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('model.pkl', 'rb'))\n",
    "print_score(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.joblib']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.remove('model.joblib')\n",
    "from joblib import dump\n",
    "\n",
    "dump(m, 'model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp model.joblib "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pickle dataframe with categories for ***apply_cats()***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df_raw[0:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.Wilderness_Area.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.Wilderness_Area.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.to_pickle(\"./df_raw.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickled_df_raw = pd.read_pickle(\"./df_raw.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickled_df_raw.Wilderness_Area.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_cats(df_test,unpickled_df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.Wilderness_Area.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.Wilderness_Area.cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess test dataset\n",
    "\n",
    "TODO - serialize nas if missing values\\\n",
    "TODO - serialize mappers if standardizing/normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred, y_pred, nas = proc_df(df_test, 'Cover_Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.score(df_pred,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.predict(df_pred).tolist()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test serialization of preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import TrainedCategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = TrainedCategories(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(t, open('preprocessor.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://./preprocessor.pkl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  3.4 MiB/  3.4 MiB]                                                \n",
      "Operation completed over 1 objects/3.4 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp ./preprocessor.pkl gs://my-custom-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_TrainedCategories = pickle.load(open('preprocessor.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_df=loaded_TrainedCategories.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_df.Wilderness_Area.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.remove('preprocess.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test create dataframe from column names and list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = loaded_df.columns\n",
    "cols = pd.Index.drop(cols,'Cover_Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [1979.0, 4.0, 38.0, 30.0, 19.0, 90.0, 146.0, 142.0, 113.0, 212.0, \"Cache\", \"C4703\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_re = pd.DataFrame(lst,cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test predictor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predictor import MyPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyPredictor.from_path(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = MyPredictor.from_path(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instances = [1979.0, 4.0, 38.0, 30.0, 19.0, 90.0, 146.0, 142.0, 113.0, 212.0, 'Cache', 'C4703']\n",
    "test_cols = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area', 'Soil_Type']\n",
    "pd.DataFrame([test_instances],columns=test_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "writing my_custom_code.egg-info/PKG-INFO\n",
      "writing dependency_links to my_custom_code.egg-info/dependency_links.txt\n",
      "writing requirements to my_custom_code.egg-info/requires.txt\n",
      "writing top-level names to my_custom_code.egg-info/top_level.txt\n",
      "reading manifest file 'my_custom_code.egg-info/SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "writing manifest file 'my_custom_code.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "creating my_custom_code-2.1\n",
      "creating my_custom_code-2.1/fastai_custom\n",
      "creating my_custom_code-2.1/my_custom_code.egg-info\n",
      "copying files to my_custom_code-2.1...\n",
      "copying MANIFEST.in -> my_custom_code-2.1\n",
      "copying predictor.py -> my_custom_code-2.1\n",
      "copying preprocess.py -> my_custom_code-2.1\n",
      "copying requirements.txt -> my_custom_code-2.1\n",
      "copying setup.py -> my_custom_code-2.1\n",
      "copying fastai_custom/__init__.py -> my_custom_code-2.1/fastai_custom\n",
      "copying fastai_custom/imports.py -> my_custom_code-2.1/fastai_custom\n",
      "copying fastai_custom/structured.py -> my_custom_code-2.1/fastai_custom\n",
      "copying my_custom_code.egg-info/PKG-INFO -> my_custom_code-2.1/my_custom_code.egg-info\n",
      "copying my_custom_code.egg-info/SOURCES.txt -> my_custom_code-2.1/my_custom_code.egg-info\n",
      "copying my_custom_code.egg-info/dependency_links.txt -> my_custom_code-2.1/my_custom_code.egg-info\n",
      "copying my_custom_code.egg-info/requires.txt -> my_custom_code-2.1/my_custom_code.egg-info\n",
      "copying my_custom_code.egg-info/top_level.txt -> my_custom_code-2.1/my_custom_code.egg-info\n",
      "Writing my_custom_code-2.1/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'my_custom_code-2.1' (and everything under it)\n"
     ]
    }
   ],
   "source": [
    "!python setup.py sdist --formats=gztar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -tvf ./dist/my_custom_code-0.1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ./dist/my_custom_code-1.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload source dist containing ***predictor.py*** and ***preprocess.py*** to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://dist/my_custom_code-2.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  8.7 KiB/  8.7 KiB]                                                \n",
      "Operation completed over 1 objects/8.7 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp dist/my_custom_code-2.1.tar.gz gs://my-custom-code/staging-dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp requirements.txt gs://my-custom-code/staging-dir/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AI Platform ***model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform models create fastai_model \\\n",
    "  --regions europe-west1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!PROJECT_ID='sk-kfp'\n",
    "!BUCKET_NAME=\"my-custom-code\"\n",
    "\n",
    "!REGION='europe-west1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copy MODEL to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://./model.joblib [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "| [1 files][177.6 MiB/177.6 MiB]                                                \n",
      "Operation completed over 1 objects/177.6 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp ./model.joblib gs://my-custom-code/model.joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install google-cloud-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME='fastai_model'\n",
    "MODEL_DIR='gs://my-custom-code/'\n",
    "CUSTOM_CODE_PATH='gs://my-custom-code/staging-dir/my_custom_code-2.1.tar.gz'\n",
    "PREDICTOR_CLASS='predictor.MyPredictor'\n",
    "VERSION_NAME='v2_1'\n",
    "#gcloud components install beta\n",
    "\n",
    "gcloud beta ai-platform versions create $VERSION_NAME \\\n",
    "  --model=$MODEL_NAME \\\n",
    "  --origin=$MODEL_DIR \\\n",
    "  --runtime-version=1.15 \\\n",
    "  --python-version=3.7 \\\n",
    "  --package-uris=$CUSTOM_CODE_PATH \\\n",
    "  --prediction-class=$PREDICTOR_CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list > requirements_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "required=''\n",
    "\n",
    "with open('requirement_works.txt') as f:\n",
    "    required = f.read().splitlines()\n",
    "    del required[0:2]\n",
    "    req_list = []\n",
    "    for item in required:\n",
    "        req_list.append(item.split(' ')[0]+'=='+item.split(' ')[-1])\n",
    "        #req_list.append(item.split(' ')[0])\n",
    "#     req_list.remove('anaconda-project')\n",
    "#     req_list.remove('anaconda-client')\n",
    "#     req_list.remove('conda-package-handling')\n",
    "#     req_list.remove('conda')\n",
    "#     req_list.remove('jupyter-aihub-deploy-extension')\n",
    "#     req_list.remove('mkl-fft')\n",
    "    with open('requirements.txt','w') as f:\n",
    "        f.write(\"\\n\".join(req_list))\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
